# radar-vision-fusion-for-ship-tracking
This study proposes a weakly supervised representation learning-based cross-modal fusion framework to enable feature-level interaction while reducing reliance on manual annotations. Specifically, radar and visual measurements are processed as ship motion-related time series, forming the radar and visual feature inputs. A pseudo-labelling strategy that combines geometric and motion consistency with signal confidence is designed to generate high-confidence radar-vision pairs. These pairs guide a Transformer-based dual-encoder that maps the processed features into a shared latent space. A novel ordinal loss is incorporated alongside a contrastive loss to optimise the network parameters under physical constraints. The fused dual-modal information is overlaid onto the video frames. This visualisation provides bridge authorities with intuitive awareness of waterway traffic to support decision-making for shipâ€“bridge collision early warning.
